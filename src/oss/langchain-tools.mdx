---
title: Tools
---

## Overview

Tools are components that Agents call to perform actions. They extend a model's capabilities beyond text by letting it interact with the world through well-defined inputs and outputs.

## Creating tools

### Basic tool definition

The simplest way to create a tool is with the `@tool` decorator. By default, the function's docstring becomes the tool's description that helps the model understand when to use it:

```python wrap
from langchain_core.tools import tool

@tool
def search_database(query: str, limit: int = 10) -> str:
    """Search the customer database for records matching the query.
    
    Args:
        query: Search terms to look for
        limit: Maximum number of results to return
    """
    return f"Found {limit} results for '{query}'"
```

Type hints are **required** as they define the tool's input schema. The docstring should be informative and concise to help the model understand the tool's purpose.

### Customizing tool properties

#### Custom tool name

By default, the tool name comes from the function name. Override it when you need something more descriptive:

```python wrap
@tool("web_search")  # Custom name
def search(query: str) -> str:
    """Search the web for information."""
    return f"Results for: {query}"

print(search.name)  # web_search
```

#### Custom tool description

Override the auto-generated tool description for clearer model guidance:

```python wrap
@tool(name="calculator", description="Performs arithmetic calculations. Use this for any math problems.")
def calc(expression: str) -> str:
    """Evaluate mathematical expressions."""
    return str(eval(expression))
```

### Advanced schema definition

For complex inputs, use Pydantic models, dataclasses, or typed dictionaries to define detailed schemas:

<CodeGroup>

```python wrap Pydantic model
from pydantic import BaseModel, Field
from typing import Literal

class WeatherInput(BaseModel):
    """Input for weather queries."""
    location: str = Field(description="City name or coordinates")
    units: Literal["celsius", "fahrenheit"] = Field(
        default="celsius",
        description="Temperature unit preference"
    )
    include_forecast: bool = Field(
        default=False,
        description="Include 5-day forecast"
    )

@tool(args_schema=WeatherInput)
def get_weather(location: str, units: str = "celsius", include_forecast: bool = False) -> str:
    """Get current weather and optional forecast."""
    temp = 22 if units == "celsius" else 72
    result = f"Current weather in {location}: {temp} degrees {units[0].upper()}"
    if include_forecast:
        result += "\nNext 5 days: Sunny"
    return result
```

```python wrap Dataclass
from dataclasses import dataclass
from typing import Literal

@dataclass
class WeatherInput:
    """Input for weather queries."""
    location: str  # City name or coordinates
    units: Literal["celsius", "fahrenheit"] = "celsius"  # Temperature unit preference
    include_forecast: bool = False  # Include 5-day forecast

@tool(args_schema=WeatherInput)
def get_weather(location: str, units: str = "celsius", include_forecast: bool = False) -> str:
    """Get current weather and optional forecast."""
    temp = 22 if units == "celsius" else 72
    result = f"Current weather in {location}: {temp} degrees {units[0].upper()}"
    if include_forecast:
        result += "\nNext 5 days: Sunny"
    return result
```

```python wrap Typed dictionary
from typing import TypedDict

class WeatherInput(TypedDict):
    """Input for weather queries."""
    location: str  # City name or coordinates
    units: Literal["celsius", "fahrenheit"] = "celsius"  # Temperature unit preference
    include_forecast: bool = False  # Include 5-day forecast

@tool(args_schema=WeatherInput)
def get_weather(location: str, units: str = "celsius", include_forecast: bool = False) -> str:
    """Get current weather and optional forecast."""
    temp = 22 if units == "celsius" else 72
    result = f"Current weather in {location}: {temp} degrees {units[0].upper()}"
    if include_forecast:
        result += "\nNext 5 days: Sunny"
    return result
```

</CodeGroup>

## Using tools with LLMs

Tools allow models to interact with the world via well defined inputs and outputs. 

<Accordion title="TODO: Move to models page">

### Direct tool binding

The simplest integration is binding tools directly to a model. The model can then decide whether to use tools based on the conversation:

```python wrap
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-5")
model_with_tools = model.bind_tools([search_database, get_weather])

response = model_with_tools.invoke("What's the weather in Sydney?")
```

### Understanding tool calls

When a model calls a tool, the response contains structured tool call information:

```python wrap
response = model_with_tools.invoke("Search for customers named John")

# The response contains tool_calls when tools are invoked
if response.tool_calls:
    tool_call = response.tool_calls[0]
    # Structure:
    # {
    #     'id': 'unique_call_id',
    #     'function': {
    #         'name': 'search_database',
    #         'arguments': '{"query": "John", "limit": 10}'
    #     },
    #     'type': 'function'
    # }
```

### Forcing tool usage

A key principle of tool calling is that the model decides when to use a tool based on the input's relevance. Sometimes you want to ensure a tool is used:

```python wrap
# Force specific tool use, even for simple math
model_forced = model.bind_tools([calculate], tool_choice="calculate")
response = model_forced.invoke("What's 2+2?")

# Force the model to use any tool
model_any = model.bind_tools([search_database, calculate], tool_choice="any")
response = model_any.invoke("Hello")
```

</Accordion>

<Tip>
To learn more about how to use tools with models, see [Models](./langchain-models).
</Tip>

## Using tools with agents

Agents go beyond simple tool binding by adding reasoning loops, state management, and multi-step execution.

<Tip>
To see examples of how to use tools with agents, see [Agents](./langchain-agents).
</Tip>

## Advanced tool patterns

### ToolNode

ToolNode is a built-in LangGraph component that handles tool calls within an agent's workflow. It works seamlessly with `create_react_agent`, offering advanced tool execution control, built in parallelism, and error handling.

#### Configuration options

`ToolNode` accepts the following parameters:

```python wrap
from langgraph.prebuilt import ToolNode

tool_node = ToolNode(
    tools=[...],              # List of tools or callables
    name="tools",             # Node name (default: "tools")
    tags=["tool-execution"],  # Optional metadata tags for filtering/organization
    handle_tool_errors=True,  # Error handling configuration (see below)
    messages_key="messages"   # Key for messages in state (default: "messages")
)
```

<ParamField path="tools" required>
  A list of tools that this node can execute. Can include:
  - LangChain `@tool` decorated functions
  - Callable objects (e.g. functions) with proper type hints and a docstring
</ParamField>

<ParamField path="name">
  Identifier for this node in your agent workflow.
  Used for debugging and visualization â€” appears in execution traces and logs.
  Default: `"tools"`
</ParamField>

<ParamField path="tags">
  Optional list of string labels for categorization.
  Useful for filtering execution traces.
  Example: ["api-calls", "external-tools"]
  Default: None
</ParamField>

<ParamField path="handle_tool_errors">
  Controls how tool execution failures are handled.
  Can be:
    - `bool`
    - `str`
    - `Callable[..., str]`
    - `type[Exception]`
    - `tuple[type[Exception], ...]`
  
  See [Error handling strategies](#error-handling-strategies) below for details.
  Default: internal `_default_handle_tool_errors`
</ParamField>

<ParamField path="messages_key">
  The state dictionary key where messages are stored.
  `ToolNode` looks here for the AI's tool calls and adds the tool response messages.
  Must match your agent's state schema.
  Default: "messages"
</ParamField>

#### Error handling strategies

`ToolNode` provides built-in error handling for tool execution through its `handle_tool_errors` property.

To customize the error handling behavior, you can configure `handle_tool_errors` to either be a boolean, a string, a callable, an exception type, or a tuple of exception types:

- **`True`**: Catch all errors and return a ToolMessage with the default error template containing the exception details.
- **`str`**: Catch all errors and return a ToolMessage with this custom error message string.
- **`type[Exception]`**: Only catch exceptions with the specified type and return the default error message for it.
- **`tuple[type[Exception], ...]`**: Only catch exceptions with the specified types and return default error messages for them.
- **`Callable[..., str]`**: Catch exceptions matching the callable's signature and return the string result of calling it with the exception.
- **`False`**: Disable error handling entirely, allowing exceptions to propagate.

`handle_tool_errors` defaults to a callable `_default_handle_tool_errors` that:
- catches tool invocation errors `ToolInvocationError` (due to invalid arguments provided by the model) and returns a descriptive error message
- ignores tool execution errors (they will be re-raised with the template string `TOOL_CALL_ERROR_TEMPLATE = "Error: {error}\n Please fix your mistakes."`)

Examples of how to use the different error handling strategies:

```python wrap
# Retry on all exception types with the default error message template string
tool_node = ToolNode(tools=[my_tool], handle_tool_errors=True)

# Retry on all exception types with a custom message string
tool_node = ToolNode(
    tools=[my_tool],
    handle_tool_errors="I encountered an issue. Please try rephrasing your request."
)

# Retry on ValueError with a custom message, otherwise raise
def handle_errors(e: ValueError) -> str:
    return "Invalid input provided"

tool_node = ToolNode([my_tool], handle_tool_errors=handle_errors)

# Retry on ValueError and KeyError with the default error message template string, otherwise raise
tool_node = ToolNode(
    tools=[my_tool],
    handle_tool_errors=(ValueError, KeyError)
)
```

#### Using with `create_react_agent`

Pass a configured `ToolNode` directly to `create_react_agent`:

```python wrap
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import ToolNode, create_react_agent

@tool
def search_database(query: str) -> str:
    """Search the database."""
    return f"Results for: {query}"

@tool
def send_email(to: str, subject: str, body: str) -> str:
    """Send an email."""
    return f"Email sent to {to}"

# Configure ToolNode with custom error handling
tool_node = ToolNode(
    tools=[search_database, send_email],
    name="email_tools",
    handle_tool_errors="Encountered an issue. Let me try a different approach."
)

# Create agent with the configured ToolNode
agent = create_react_agent(
    model=ChatOpenAI(model="gpt-4"),
    tools=tool_node,  # Pass ToolNode instead of tools list
    prompt="You are a helpful email assistant."
)

# The agent will use your custom ToolNode configuration
result = agent.invoke({
    "messages": [{"role": "user", "content": "Search for John and email him"}]
})
```

When you pass a `ToolNode` to `create_react_agent`, the agent uses your exact configuration including error handling, custom names, and tags. This is useful when you need fine-grained control over tool execution behavior.

### Accessing agent state inside a tool

Tools can access the current graph state using the `InjectedState` annotation:

```python wrap
from typing_extensions import Annotated
from langgraph.prebuilt import InjectedState

# Access the current conversation state
@tool
def summarize_conversation(
    state: Annotated[dict, InjectedState]
) -> str:
    """Summarize the conversation so far."""
    messages = state["messages"]
    
    human_msgs = sum(1 for m in messages if m.__class__.__name__ == "HumanMessage")
    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == "AIMessage")
    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == "ToolMessage")
    
    return f"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results"

# Access custom state fields
@tool
def get_user_preference(
    pref_name: str,
    preferences: Annotated[dict, InjectedState("user_preferences")]  # InjectedState parameters are not visible to the model
) -> str:
    """Get a user preference value."""
    return preferences.get(pref_name, "Not set")
```
**Important:** State-injected arguments are hidden from the model. For the example above, the model only sees `pref_name` in the tool schema - `preferences` is *not* included in the request.

### Updating agent state inside a tool

Use a tool that returns a `Command` to update the agent state:

```python wrap
from langgraph.types import Command
from langchain_core.messages import ToolMessage
from langchain_core.tools import InjectedToolCallId
from langgraph.graph.message import RemoveMessage, REMOVE_ALL_MESSAGES

# Update the conversation history by removing all messages
@tool
def clear_conversation() -> Command:
    """Clear the conversation history."""
    
    return Command(
        update={
            "messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],
        }
    )

# Update the user_name in the agent state
@tool
def update_user_name(
    new_name: str,
    tool_call_id: Annotated[dict, InjectedToolCallId]
) -> Command:
    """Update the user's name."""
    return Command(update={"user_name": new_name})
```

### Accessing runtime context inside a tool

Tools may sometimes require immutable context data, such as runtime-only arguments (e.g., user IDs or session details), that should not be controlled by the model. This can be achieved with `get_runtime`:

```python wrap
from dataclasses import dataclass
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool
from langgraph.runtime import get_runtime

@dataclass
class ContextSchema:
    user_name: str

@tool
def get_user_name(
) -> str:
    """Get the user's name."""
    runtime = get_runtime(ContextSchema)
    return runtime.context.user_name

model = ChatOpenAI(model="gpt-4o")
agent = create_react_agent(
    model,
    tools=[get_user_name],
    context_schema=ContextSchema
)
agent.invoke(
    {"messages": [{"role": "user", "content": "What is my name?"}]},
    context=ContextSchema(user_name="John Smith")
)
```

### Accessing long-term memory inside a tool

Tools may also require access to your agent's long-term memory store, e.g. user-specific or application-specific data stored across conversations. This can be achieved by using `get_store`:

```python wrap
from langgraph.config import get_store

@tool
def get_user_info(user_id: str) -> str:
    """Look up user info."""
    store = get_store()
    user_info = store.get(("users",), user_id)
    return str(user_info.value) if user_info else "Unknown user"
```

### Updating long-term memory inside a tool

To update long-term memory, you can use the `.put()` method of `InMemoryStore`. A full example of persistent memory across sessions:

```python wrap expandable
from typing import Any
from langgraph.prebuilt import InjectedStore
from langgraph.config import get_store
from langgraph.store.memory import InMemoryStore

@tool
def get_user_info(user_id: str) -> str:
    """Look up user info."""
    store = get_store()
    user_info = store.get(("users",), user_id)
    return str(user_info.value) if user_info else "Unknown user"

@tool
def save_user_info(user_id: str, user_info: dict[str, Any]) -> str:
    """Save user info."""
    store = get_store()
    store.put(("users",), user_id, user_info)
    return "Successfully saved user info."

store = InMemoryStore()
agent = create_react_agent(
    model,
    tools=[get_user_info, save_user_info],
    store=store
)

# First session: save user info
agent.invoke({
    "messages": [{"role": "user", "content": "Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev"}]
})

# Second session: get user info
agent.invoke({
    "messages": [{"role": "user", "content": "Get user info for user with id 'abc123'"}]
})
# Here is the user info for user with ID "abc123":
# - Name: Foo
# - Age: 25
# - Email: foo@langchain.dev
```
