---
title: Model Context Protocol (MCP)
---

import AlphaCallout from '/snippets/alpha-lc-callout.mdx';

<AlphaCallout />

:::python
[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library.
:::
:::js
[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters/adapters) library.
:::

## Install

:::python
Install the `langchain-mcp-adapters` library to use MCP tools in LangGraph:

<CodeGroup>
```bash pip
pip install langchain-mcp-adapters
```

```bash uv
uv add langchain-mcp-adapters
```
</CodeGroup>
:::

:::js
Install the `@langchain/mcp-adapters` library to use MCP tools in LangGraph:

<CodeGroup>
```bash npm
npm install @langchain/mcp-adapters
```

```bash pnpm
pnpm add @langchain/mcp-adapters
```

```bash yarn
yarn add @langchain/mcp-adapters
```

```bash bun
bun add @langchain/mcp-adapters
```
</CodeGroup>
:::

## Transport types

MCP supports different transport mechanisms for client-server communication:

- **Streamable HTTP**: Connect to a remote MCP server over HTTP. Best for production solutions.
- **stdio**: Client launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.

## Use MCP tools

:::python
`langchain-mcp-adapters` enables agents to use tools defined across one or more MCP server.

```python Accessing multiple MCP servers icon="server"
from langchain_mcp_adapters.client import MultiServerMCPClient # [!code highlight]
from langchain.agents import create_agent

client = MultiServerMCPClient( # [!code highlight]
    {
        "math": {
            "transport": "stdio",  # Local subprocess communication
            "command": "python",
            # Absolute path to your math_server.py file
            "args": ["/path/to/math_server.py"],
        },
        "weather": {
            "transport": "streamable_http",  # HTTP-based remote server
            # Ensure you start your weather server on port 8000
            "url": "http://localhost:8000/mcp",
        }
    }
)

tools = await client.get_tools() # [!code highlight]
agent = create_agent(
    "anthropic:claude-3-7-sonnet-latest",
    tools # [!code highlight]
)
math_response = await agent.ainvoke(
    {"messages": [{"role": "user", "content": "what's (3 + 5) x 12?"}]}
)
weather_response = await agent.ainvoke(
    {"messages": [{"role": "user", "content": "what is the weather in nyc?"}]}
)
```
:::

:::js
`@langchain/mcp-adapters` enables agents to use tools defined across one or more MCP server.

```ts Accessing multiple MCP servers icon="server"
import { MultiServerMCPClient } from "@langchain/mcp-adapters"; // [!code highlight]
import { ChatAnthropic } from "@langchain/anthropic";
import { createAgent } from "langchain";

const client = new MultiServerMCPClient({ // [!code highlight]
    math: {
        transport: "stdio",  // Local subprocess communication
        command: "node",
        // Replace with absolute path to your math_server.js file
        args: ["/path/to/math_server.js"],
    },
    weather: {
        transport: "sse",  // Server-Sent Events for streaming
        // Ensure you start your weather server on port 8000
        url: "http://localhost:8000/mcp",
    },
});

const tools = await client.getTools(); // [!code highlight]
const agent = createAgent({ 
    model: new ChatAnthropic({ model: "claude-3-7-sonnet-latest" }),
    tools: tools, // [!code highlight]
});

const mathResponse = await agent.invoke({
    messages: [{ role: "user", content: "what's (3 + 5) x 12?" }],
});

const weatherResponse = await agent.invoke({
    messages: [{ role: "user", content: "what is the weather in nyc?" }],
});
```
:::

<Note>
    `MultiServerMCPClient` is **stateless by default**. Each tool invocation creates a fresh MCP `ClientSession`, executes the tool, and then cleans up.
</Note>

## Custom MCP servers

:::python

To create a custom MCP serverm you can use the [FastMCP](https://gofastmcp.com/getting-started/welcome) library.


<CodeGroup>
```bash pip
pip install fastmcp
```

```bash uv
uv add fastmcp
```
</CodeGroup>
:::

:::js
To create your own MCP servers, you can use the `@modelcontextprotocol/sdk` library. This library provides a simple way to define [tools](https://modelcontextprotocol.io/docs/learn/server-concepts#tools-ai-actions) and run them as servers.

<CodeGroup>
```bash npm
npm install @modelcontextprotocol/sdk
```

```bash pnpm
pnpm add @modelcontextprotocol/sdk
```

```bash yarn
yarn add @modelcontextprotocol/sdk
```

```bash bun
bun add @modelcontextprotocol/sdk
```
</CodeGroup>
:::

Use the following reference implementations to test your agent with MCP tool servers.


:::python
<CodeGroup>
```python title="Math server (stdio transport)" icon="floppy-disk"
from fastmcp import FastMCP

mcp = FastMCP("Math")

@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b

@mcp.tool()
def multiply(a: int, b: int) -> int:
    """Multiply two numbers"""
    return a * b

if __name__ == "__main__":
    mcp.run(transport="stdio")
```

```python title="Weather server (streamable HTTP transport)" icon="wifi"
from fastmcp import FastMCP

mcp = FastMCP("Weather")

@mcp.tool()
async def get_weather(location: str) -> str:
    """Get weather for location."""
    return "It's always sunny in New York"

if __name__ == "__main__":
    mcp.run(transport="streamable-http")
```
</CodeGroup>
:::

:::js
```typescript title="Math server (stdio transport)" icon="floppy-disk"
import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import {
    CallToolRequestSchema,
    ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";

const server = new Server(
    {
        name: "math-server",
        version: "0.1.0",
    },
    {
        capabilities: {
            tools: {},
        },
    }
);

server.setRequestHandler(ListToolsRequestSchema, async () => {
    return {
        tools: [
        {
            name: "add",
            description: "Add two numbers",
            inputSchema: {
                type: "object",
                properties: {
                    a: {
                        type: "number",
                        description: "First number",
                    },
                    b: {
                        type: "number",
                        description: "Second number",
                    },
                },
                required: ["a", "b"],
            },
        },
        {
            name: "multiply",
            description: "Multiply two numbers",
            inputSchema: {
                type: "object",
                properties: {
                    a: {
                        type: "number",
                        description: "First number",
                    },
                    b: {
                        type: "number",
                        description: "Second number",
                    },
                },
                required: ["a", "b"],
            },
        },
        ],
    };
});

server.setRequestHandler(CallToolRequestSchema, async (request) => {
    switch (request.params.name) {
        case "add": {
            const { a, b } = request.params.arguments as { a: number; b: number };
            return {
                content: [
                {
                    type: "text",
                    text: String(a + b),
                },
                ],
            };
        }
        case "multiply": {
            const { a, b } = request.params.arguments as { a: number; b: number };
            return {
                content: [
                {
                    type: "text",
                    text: String(a * b),
                },
                ],
            };
        }
        default:
            throw new Error(`Unknown tool: ${request.params.name}`);
    }
});

async function main() {
    const transport = new StdioServerTransport();
    await server.connect(transport);
    console.error("Math MCP server running on stdio");
}

main();
```

```typescript title="Weather server (SSE transport)" icon="wifi"
import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { SSEServerTransport } from "@modelcontextprotocol/sdk/server/sse.js";
import {
    CallToolRequestSchema,
    ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";
import express from "express";

const app = express();
app.use(express.json());

const server = new Server(
    {
        name: "weather-server",
        version: "0.1.0",
    },
    {
        capabilities: {
            tools: {},
        },
    }
);

server.setRequestHandler(ListToolsRequestSchema, async () => {
    return {
        tools: [
        {
            name: "get_weather",
            description: "Get weather for location",
            inputSchema: {
            type: "object",
            properties: {
                location: {
                type: "string",
                description: "Location to get weather for",
                },
            },
            required: ["location"],
            },
        },
        ],
    };
});

server.setRequestHandler(CallToolRequestSchema, async (request) => {
    switch (request.params.name) {
        case "get_weather": {
            const { location } = request.params.arguments as { location: string };
            return {
                content: [
                    {
                        type: "text",
                        text: `It's always sunny in ${location}`,
                    },
                ],
            };
        }
        default:
            throw new Error(`Unknown tool: ${request.params.name}`);
    }
});

app.post("/mcp", async (req, res) => {
    const transport = new SSEServerTransport("/mcp", res);
    await server.connect(transport);
});

const PORT = process.env.PORT || 8000;
app.listen(PORT, () => {
    console.log(`Weather MCP server running on port ${PORT}`);
});
```
:::


## Advanced

### Modify tool request at run time

If you need to change any part of the tool request at runtime (e.g., modify headers):

* Example of how to change headers (e.g., to provide some metadata based on `config`)

### Access MCP logs

The MCP protocol [allows servers to send log messages to client](https://modelcontextprotocol.io/specification/2025-03-26/server/utilities/logging#log-levels ).

### Progress notification

Server side tools can send progress updates to clients using the MCP [progress utility](https://modelcontextprotocol.io/specification/2025-03-26/basic/utilities/progress).

:::python
To subscribe to progress updates, use the `on_progress` callback:
:::

### Implementing auth

:::python
The `langchain-mcp-adapters` library uses the official [MCP SDK](https://github.com/modelcontextprotocol/python-sdk) under the hood, which allows you to provide a custom authentication mechanism by implementing the `httpx.Auth` interface.

```python
from langchain_mcp_adapters.client import MultiServerMCPClient

client = MultiServerMCPClient(
    {
        "weather": {
            "transport": "streamable_http",
            "url": "http://localhost:8000/mcp",
            "auth": auth, # [!code highlight]
        }
    }
)
```


* [Example custom auth implementation](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/clients/simple-auth-client/mcp_simple_auth_client/main.py)
* [Built-in OAuth flow](https://github.com/modelcontextprotocol/python-sdk/blob/main/src/mcp/client/auth.py#L179)
:::

### Interacting with stateful servers

If you need to control the lifecycle of the MCP session (e.g., you want to interact with a stateful server that maintains context between tool calls), you can create a persistent `ClientSession` using `client.session()`.

This allows you to manage the session [lifecycle explicitly](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle) and will go through the full MCP session lifecycle  (initializing a session, using the session and then closing the session).

:::python
```python Using MCP ClientSession for stateful tool usage
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain.agents import create_agent

client = MultiServerMCPClient({...})

# Create a session explicitly
async with client.session() as session: # [!code highlight]
    # Pass the session to load tools
    tools = await load_mcp_tools(session) # [!code highlight]
    agent = create_agent(
        "anthropic:claude-3-7-sonnet-latest",
        tools
    )
```
:::

:::js
```typescript Using MCP ClientSession for stateful tool usage
import { loadMCPTools } from "@langchain/mcp-adapters/tools.js";
import { createAgent } from "langchain";

const client = new MultiServerMCPClient({...});
const session = await client.session("some_server");
const tools = await loadMCPTools(session);
const agent = createAgent({
    model: new ChatAnthropic({ model: "claude-3-7-sonnet-latest" }),
    tools,
});
```
:::

<Important>
    If you open long-lived sessions which maintain state and are trying to use your agent in a conversational capacity
    or have human in the loop and are serving it via a web server.
</Important>

## Additional resources

* [MCP documentation](https://modelcontextprotocol.io/introduction)
* [MCP Transport documentation](https://modelcontextprotocol.io/docs/concepts/transports)
:::python
* [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters)
:::
:::js
* [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters/)
:::
