---
title: Models
---

import ConversationHistory from "/snippets/oss/langchain-conversation-history.mdx";

Large Language Models (LLMs) are advanced machine learning models that can interpret and generate text like humans do. They're versatile enough to write content, translate languages, summarize information, and answer questions without needing special training for each task.

Providers like OpenAI, Anthropic, and Google offer access to a range of LLMs each carrying their own strengths and weaknesses. Every provider has its own unique interaction patterns for accessing and using the models, typically through a user interface or an API where [messages]() are exchanged between a user and the model.

In addition to text generation, many models support other interaction patterns, such as:

- [Tool calling](#tool-calling) - where models can request execution of external tools (such as database queries or API calls) and incorporate the results into their responses.
- [Structured output](#structured-output) - where the model is constrained to generate JSON output that matches a provided schema.
- [Multimodal](#multimodal) - where models can process and return data other than text, such as images, audio, and video.
- [Reasoning](#reasoning) - where models are able to perform multi-step reasoning to arrive at a conclusion.

When working with models, each provider typically has its own unique format to represent the inputs and outputs of a model, which can make it difficult to swap between models and providers without significant code changes.

LangChain simplifies this by providing a common `ChatModel` abstraction for interacting with LLMs across all supported models & providers. If you're experimenting with different models, or building production agents, this lets you focus on the logic of your application rather than the implementation details of the underlying model.

---

## Basic usage

:::python

### `init_chat_model()`

The easiest way to get started with a model in LangChain is to use `init_chat_model` to initialize one from a model [provider]() of your choice.

<Tabs>
  <Tab title="OpenAI" icon="/images/vendor/openai-light.svg">
    ```python
    from langchain.chat_models import init_chat_model

    model = init_chat_model(
      model="gpt-5",
      model_provider="openai"
    )
    response = model.invoke("Why do parrots talk?")
    ````

  </Tab>
  <Tab title="Anthropic" icon="/images/vendor/claude.svg">
    ```python
    from langchain.chat_models import init_chat_model

    model = init_chat_model(
      model="claude-opus-4-0",
      model_provider="anthropic"
    )
    response = model.invoke("Why do parrots talk?")
    ````

  </Tab>
  <Tab title="Google" icon="/images/vendor/gemini.svg">
    ```python
    from langchain.chat_models import init_chat_model

    model = init_chat_model(
      model="gemini-2.5-pro",
      model_provider="google"
    )
    response = model.invoke("Why do parrots talk?")
    ````

  </Tab>
  <Tab title="AWS Bedrock" icon="/images/vendor/aws-bedrock.png">
    ```python
    from langchain.chat_models import init_chat_model

    model = init_chat_model(
      model="anthropic.claude-opus-4",
      model_provider="bedrock"
    )
    response = model.invoke("Why do parrots talk?")
    ````

  </Tab>
  <Tab title="Ollama" icon="/images/vendor/ollama.svg">
    ```python
    from langchain.chat_models import init_chat_model

    model = init_chat_model(
      model="gpt-oss",
      model_provider="ollama"
    )
    response = model.invoke("Why do parrots talk?")
    ````

  </Tab>
</Tabs>
:::

:::js

### `initChatModel()`

The easiest way to get started with a model in LangChain is to use `initChatModel` to initialize one from a model [provider]() of your choice.

<Tabs>
  <Tab title="OpenAI" icon="/images/vendor/openai-light.svg">
    ```typescript
    import { initChatModel } from "langchain/chat_models";

    const model = await initChatModel(
      "gpt-5",
      { modelProvider: "openai" }
    );
    const response = await model.invoke("Why do parrots talk?");
    ````

  </Tab>
  <Tab title="Anthropic" icon="/images/vendor/claude.svg">
    ```typescript
    import { initChatModel } from "langchain/chat_models";

    const model = await initChatModel(
      "claude-opus-4-0",
      { modelProvider: "anthropic" }
    );
    const response = await model.invoke("Why do parrots talk?");
    ````

  </Tab>
  <Tab title="Google" icon="/images/vendor/gemini.svg">
    ```typescript
    import { initChatModel } from "langchain/chat_models";

    const model = await initChatModel(
      "gemini-2.5-pro",
      { modelProvider: "google" }
    );
    const response = await model.invoke("Why do parrots talk?");
    ````

  </Tab>
  <Tab title="AWS Bedrock" icon="/images/vendor/aws-bedrock.png">
    ```typescript
    import { initChatModel } from "langchain/chat_models";

    const model = await initChatModel(
      "anthropic.claude-opus-4",
      { modelProvider: "bedrock" }
    );
    const response = await model.invoke("Why do parrots talk?");
    ````

  </Tab>
  <Tab title="Ollama" icon="/images/vendor/ollama.svg">
    ```typescript
    import { initChatModel } from "langchain/chat_models";

    const model = await initChatModel(
      "gpt-oss",
      { modelProvider: "ollama" }
    );
    const response = await model.invoke("Why do parrots talk?");
    ````

  </Tab>
</Tabs>
:::

In addition to passing in single string messages, you can provide the model a
list of messages to represent conversation history. The role of each message
provides context to the model about who is speaking. See the [messages]() guide
for more info.

:::python

```python Conversation history
from langchain.messages import HumanMessage, AIMessage, SystemMessage

conversation = [
    SystemMessage("You are a helpful assistant that translates English to French."),
    HumanMessage("Translate: I love programming."),
    AIMessage("J'adore la programmation."),
    HumanMessage("Translate: I love building applications.")
]

response = model.invoke(conversation)
print(response)
```

:::

:::js

```js Conversation history
import {
  HumanMessage,
  AIMessage,
  SystemMessage,
} from "@langchain/core/messages";

const conversation = [
  new SystemMessage(
    "You are a helpful assistant that translates English to French."
  ),
  new HumanMessage("Translate: I love programming."),
  new AIMessage("J'adore la programmation."),
  new HumanMessage("Translate: I love building applications."),
];

const response = await model.invoke(conversation);
console.log(response);
```

:::

## Parameters

A `ChatModel` takes parameters that can be used to configure its behavior. The
full set of supported parameters vary by model and provider, but common ones
include:

:::python

<ParamField body="model" type="string" required>
  The name or identifier of the specific model you want to use with the
  provider.
</ParamField>

<ParamField body="api_key" type="string">
  The key required for authenticating with the model's provider. This is usually
  issued when you sign up for access to the model.
</ParamField>

<ParamField body="temperature" type="number">
  Controls the randomness of the model's output. A higher number makes responses
  more creative, while a lower one makes them more deterministic.
</ParamField>

<ParamField body="stop" type="string[]">
  A sequence of characters that indicates when the model should stop generating
  its output.
</ParamField>

<ParamField body="timeout" type="number">
  The maximum time (in seconds) to wait for a response from the model before
  canceling the request.
</ParamField>

<ParamField body="max_tokens" type="number">
  Limits the total number of [tokens]() in the response, effectively controlling
  how long the output can be.
</ParamField>

<ParamField body="max_retries" type="number">
  The maximum number of attempts the system will make to resend a request if it
  fails due to issues like network timeouts or rate limits.
</ParamField>
:::

:::js

<ParamField body="model" type="string" required>
  The name or identifier of the specific model you want to use with the
  provider.
</ParamField>

<ParamField body="apiKey" type="string">
  The key required for authenticating with the model's provider. This is usually
  issued when you sign up for access to the model.
</ParamField>

<ParamField body="temperature" type="number">
  Controls the randomness of the model's output. A higher number makes responses
  more creative, while a lower one makes them more deterministic.
</ParamField>

<ParamField body="stop" type="string[]">
  A sequence of characters that indicates when the model should stop generating
  its output.
</ParamField>

<ParamField body="timeout" type="number">
  The maximum time (in seconds) to wait for a response from the model before
  canceling the request.
</ParamField>

<ParamField body="maxTokens" type="number">
  Limits the total number of [tokens]() in the response, effectively controlling
  how long the output can be.
</ParamField>

<ParamField body="maxRetries" type="number">
  The maximum number of attempts the system will make to resend a request if it
  fails due to issues like network timeouts or rate limits.
</ParamField>
:::

<Info>
  To find all the parameters supported by a `ChatModel`, head to its respective
  [reference]().
</Info>

## Invocation

Models must be invoked to generate output. There are three main methods for
doing this, each suited to different use cases.

:::python

<Tip>
  Each invocation method has an
  <Tooltip tip="Asynchronous methods are useful for non-blocking operations, especially in web servers or applications that handle multiple tasks concurrently.">
    asynchronous
  </Tooltip>
  equivalent, typically prefixed with the letter `'a'` For example: `ainvoke()`,
  `astream()`, `abatch()` A full list of async methods can be found in the
  [reference]().
</Tip>
:::

### Invoke

The most straightforward way to call a model is to use `invoke()` with a single
message or a list of messages.

A list of messages can be provided to a model to represent a conversation
history. Each message has a role to provide context as to who is speaking in the
sequence. See the [messages]() guide for more detail.

:::python

<CodeGroup>

```python Single message
response = model.invoke("Why do parrots have colorful feathers?")
print(response)
```

<ConversationHistory />

</CodeGroup>
:::

:::js

<CodeGroup>

```typescript Single message
const response = await model.invoke("Why do parrots have colorful feathers?");
console.log(response);
```

<ConversationHistory />

</CodeGroup>
:::

### Stream

Most models can stream their output content while it is being generated. By
displaying output progressively, streaming significantly improves user
experience, particularly for longer responses.

Calling `stream()` returns an <Tooltip tip="An object that progressively provides access to each item of a collection, in order">iterator</Tooltip>
that yields output chunks as they are produced. You can use a loop to process
each chunk in real-time:

:::python

```python Streaming
for chunk in model.stream("Why do parrots have colorful feathers?"):
    print(chunk)
```

:::

:::js

```typescript Streaming
const stream = await model.stream("Why do parrots have colorful feathers?");
for await (const chunk of stream) {
  console.log(chunk);
}
```

:::

### Batch

Batching a collection of independent requests to a model can significantly
improve performance, as the processing can be done in parallel:

:::python

```python Batch
responses = model.batch([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
])
for response in responses:
    print(response)
```

By default, `batch()` will only return the final output for the entire batch.
If you want to receive the output for each individual input as it is finishes
generating, you can stream results with `batch_as_completed()`:

```python Yield responses upon completion
for response in model.batch_as_completed([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
]):
    print(response)
```

<Note>
  When using `batch_as_completed()`, results may arrive out of order. Each
  includes the input index for matching to reconstruct the original order if
  needed.
</Note>
:::

:::js

```typescript Batch
const responses = await model.batch([
  "Why do parrots have colorful feathers?",
  "How do airplanes fly?",
  "What is quantum computing?",
]);
for (const response of responses) {
  console.log(response);
}
```

## Advanced features

{/* ### Tool calling

Models can request to call [tools]() that perform tasks such as fetching data
from a database, searching the web, or running code.

<Note>
  You will sometimes hear the term `function calling`. We use this term
  interchangeably with `tool calling`.
</Note>

To make tools available for use by a model, they must be bound using
`.bind_tools()`. In subsequent invocations, the model can choose to call any of
the bound tools as needed. For more details on creating, using, and executing
tools requests, see the [tools]() guide.

```python Binding tools highlight={16}
from pydantic import BaseModel, Field

class GetWeather(BaseModel):
    """Get the current weather in a given location"""
    location: str = Field(..., description="The city and state, e.g. Boston, MA")

model_with_tools = model.bind_tools([GetWeather])

response = model_with_tools.invoke("What's the weather like in Boston?")
for tool_call in response.tool_calls:
    # View tool calls made by the model
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
```

If we want to include some tool info here, we can do so like this:

TODO a switcher to iterate through all different tool schema options
TODO model switcher to show this example for our top 5 providers

```python Binding tools highlight={16}
from pydantic import BaseModel, Field
from langchain.chat_models import init_chat_model

class GetWeather(BaseModel):
    """Get the current weather in a given location"""
    location: str = Field(..., description="The city and state, e.g. Boston, MA")

class Calculator(BaseModel):
    """Perform mathematical calculations"""
    expression: str = Field(..., description="Mathematical expression to evaluate")

model = init_chat_model(
    model="gpt-5",
    model_provider="openai"
)
model_with_tools = model.bind_tools([GetWeather, Calculator])

response = model_with_tools.invoke("What's the weather like in Boston and what's 25 * 4?")
for tool_call in response.tool_calls:
    # View tool calls made by the model
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
```

In this example, the model's response includes a **request** to execute a tool.
It is up to you to perform the requested action and return the result back to
the model for use in subsequent reasoning.

TODO link out to a different guide on tool execution; beyond the scope here

#### Forcing tool calls

By default, the model has the freedom to choose which bound tool to use based on
the user's input. However, you might want to force choosing a tool, ensuring the
model uses either a particular tool or _any_ tool from a given list:

<CodeGroup>

```python Force use of specific tools
response = model_with_tools.invoke(
    "What's the weather in both New York and London, and calculate 15 + 25",
    force_tool_calls=["GetWeather", "Calculator"]
)
```

```python Force use of any tool
response = model_with_tools.invoke(
    "What's the weather in both New York and London, and calculate 15 + 25",
    force_tool_calls="any"
)
```

</CodeGroup>

#### Parallel tool calls

TODO */}

### Structured outputs

Models can be requested to respond in a particular format (e.g., JSON or
matching a defined schema). This is useful for data extraction. See the
[structured outputs]() guide for more info.

### Multimodality

Certain models can process and return non-textual data such as images, audio,
and video. This is known as [multimodality]().

TODO show example of image/audio/video input

TODO show example of image/audio/video output

### Reasoning

## Advanced configuration

## Supported models

LangChain supports all major model providers, including OpenAI, Anthropic,
Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models
with different capabilities. For a full list of supported models in LangChain,
see the [provider reference]().

{/* ### Context window

A model's context window refers to the maximum size of the input that it can
process at one time. Maximum size varies from model to model.

If the input to a model exceeds the context window, the model may not be able to
process the entire input and could raise an error. In conversational
applications, this determines how much information the model can "remember"
throughout a conversation. Consequently, you may need to manage the context to
maintain a coherent dialogue. */}
