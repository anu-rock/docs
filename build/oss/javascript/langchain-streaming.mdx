---
title: Streaming
---

import AlphaCallout from '/snippets/alpha-lc-callout.mdx';

<AlphaCallout />

## Overview

Streaming is crucial for enhancing the responsiveness of applications built on LLMs.
By displaying output progressively, even before a complete response is ready, streaming significantly 
improves user experience (UX), particularly when dealing with the latency of LLMs.

## Stream from an agent

LangChain's streaming system lets you surface live feedback from agent runs to your application.
There are three main categories of data you can stream:

1. [Agent progress](/oss/javascript/langchain-streaming#agent-progress) — get state updates after each agent step.
2. [LLM tokens](/oss/javascript/langchain-streaming#llm-tokens) — stream language model tokens as they're generated.
3. [Custom updates](/oss/javascript/langchain-streaming#custom-updates) — emit user-defined signals (e.g., “Fetched 10/100 records”).

You can also stream [multiple modes](/oss/javascript/langchain-streaming#stream-multiple-modes) at once.

### Agent progress



To stream agent progress, use the [`stream()`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.CompiledStateGraph.html#stream) method with `streamMode: "updates"`. This emits an event after every agent step.


For example, if you have an agent that calls a tool once, you should see the following updates:

* **LLM node**: AI message with tool call requests
* **Tool node**: Tool message with execution result
* **LLM node**: Final AI response



```typescript
import z from "zod";
import { createReactAgent, tool } from "langchain";

const getWeather = tool(
  async ({ city }) => {
    return `The weather in ${city} is always sunny!`;
  },
  {
    name: "get_weather",
    description: "Get weather for a given city.",
    schema: z.object({
      city: z.string(),
    }),
  }
);

const agent = createReactAgent({
  model: "openai:gpt-5-nano",
  tools: [getWeather],
});

for await (const chunk of await agent.stream(
  { messages: [{ role: "user", content: "what is the weather in sf" }] },
  { streamMode: "updates" }
)) {
  const [step, content] = Object.entries(chunk)[0];
  console.log(`step: ${step}`);
  console.log(`content: ${JSON.stringify(content, null, 2)}`);
}
/**
 * step: agent
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         // ...
 *         "tool_calls": [
 *           {
 *             "name": "get_weather",
 *             "args": {
 *               "city": "San Francisco"
 *             },
 *             "type": "tool_call",
 *             "id": "call_0qLS2Jp3MCmaKJ5MAYtr4jJd"
 *           }
 *         ],
 *         // ...
 *       }
 *     }
 *   ]
 * }
 * step: tools
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         "content": "The weather in San Francisco is always sunny!",
 *         "name": "get_weather",
 *         // ...
 *       }
 *     }
 *   ]
 * }
 * step: agent
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         "content": "The latest update says: The weather in San Francisco is always sunny!\n\nIf you’d like real-time details (current temperature, humidity, wind, and today’s forecast), I can pull the latest data for you. Want me to fetch that?",
 *         // ...
 *       }
 *     }
 *   ]
 * }
 */
```


### LLM tokens



To stream tokens as they are produced by the LLM, use `streamMode: "messages"`:

```typescript
import z from "zod";
import { createReactAgent, tool } from "langchain";

const getWeather = tool(
  async ({ city }) => {
    return `The weather in ${city} is always sunny!`;
  },
  {
    name: "get_weather",
    description: "Get weather for a given city.",
    schema: z.object({
      city: z.string(),
    }),
  }
);

const agent = createReactAgent({
  model: "openai:gpt-4o-mini",
  tools: [getWeather],
});

for await (const [token, metadata] of await agent.stream(
  { messages: [{ role: "user", content: "what is the weather in sf" }] },
  { streamMode: "messages" }
)) {
  console.log(`node: ${metadata.langgraph_node}`);
  console.log(`content: ${JSON.stringify(token.contentBlocks, null, 2)}`);
}
```


### Custom updates



To stream updates from tools as they are executed, you can use the `writer` parameter from the configuration.

```typescript
import z from "zod";
import { tool, createReactAgent } from "langchain";
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const getWeather = tool(
  async (input, config: LangGraphRunnableConfig) => {
    // Stream any arbitrary data
    config.writer?.(`Looking up data for city: ${input.city}`);
    // ... fetch city data
    config.writer?.(`Acquired data for city: ${input.city}`);
    return `It's always sunny in ${input.city}!`;
  },
  {
    name: "get_weather",
    description: "Get weather for a given city.",
    schema: z.object({
      city: z.string().describe("The city to get weather for."),
    }),
  }
);

const agent = createReactAgent({
  model: "openai:gpt-4o-mini",
  tools: [getWeather],
});

for await (const chunk of await agent.stream(
  { messages: [{ role: "user", content: "what is the weather in sf" }] },
  { streamMode: "custom" }
)) {
  console.log(chunk);
}
```

```shell title="Output"
Looking up data for city: San Francisco
Acquired data for city: San Francisco
```

<Note>
  If you add the `writer` parameter to your tool, you won't be able to invoke the tool outside of a LangGraph execution context without providing a writer function.
</Note>


### Stream multiple modes



You can specify multiple streaming modes by passing streamMode as an array: `streamMode: ["updates", "messages", "custom"]`:

```typescript
import z from "zod";
import { tool, createReactAgent } from "langchain";
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const getWeather = tool(
  async (input, config: LangGraphRunnableConfig) => {
    // Stream any arbitrary data
    config.writer?.(`Looking up data for city: ${input.city}`);
    // ... fetch city data
    config.writer?.(`Acquired data for city: ${input.city}`);
    return `It's always sunny in ${input.city}!`;
  },
  {
    name: "get_weather",
    description: "Get weather for a given city.",
    schema: z.object({
      city: z.string().describe("The city to get weather for."),
    }),
  }
);

const agent = createReactAgent({
  model: "openai:gpt-4o-mini",
  tools: [getWeather],
});

for await (const [streamMode, chunk] of await agent.stream(
  { messages: [{ role: "user", content: "what is the weather in sf" }] },
  { streamMode: ["updates", "messages", "custom"] }
)) {
  console.log(`${streamMode}: ${JSON.stringify(chunk, null, 2)}`);
}
```


### Disable streaming

In some applications you might need to disable streaming of individual tokens for a given model.
This is useful in [multi-agent](/oss/javascript/multi-agent) systems to control which agents stream their output.

See the [Models](/oss/javascript/models#disable-streaming) guide to learn how to disable streaming.
