---
title: Migrate LangChain to v1.0.0
---

import AlphaCallout from '/snippets/alpha-lc-callout.mdx';

<AlphaCallout />

# Migrate LangChain to v1.0.0

This guide helps you migrate from LangChain 0.x to v1.0.0.

## Overview

LangChain v1.0.0 introduces:
- Standardized content blocks for consistent LLM I/O
- Dropped Python 3.9 support (minimum Python 3.10)

## Breaking Changes

### Python Version Requirements

**Action Required**: Upgrade to Python 3.10 or later.

```bash
# Check your Python version
python --version

# If using Python 3.9 or earlier, upgrade before installing v1.0.0
```

### Message Structure Changes

#### Content Blocks

Messages now support standardized content blocks for better multimodal handling.

**Before (v0.x):**
```python
from langchain_core.messages import AIMessage

message = AIMessage(content="Hello world")
```

**After (v1.0.0):**
```python
from langchain_core.messages import AIMessage

# Basic text message (backward compatible)
message = AIMessage(content="Hello world")

# Access typed content blocks
content_blocks = message.content_blocks  # New property
```

## New Features

### Standardized Content Blocks

LangChain v1.0.0 introduces standardized content block types:

...

### Provider Auto-Translation

When specifying `model_provider`, content is automatically normalized:

```python
from langchain_openai import ChatOpenAI

# Automatic content normalization
llm = ChatOpenAI(model_provider="openai")
response = llm.invoke("Hello")
# Content blocks are automatically standardized
```

## Migration Checklist

- [ ] **Python Version**: Ensure Python 3.10 or later
- [ ] **Update Dependencies**:

## Backward Compatibility

### Legacy Format Support


### Gradual Migration

You can migrate gradually:

1. Update dependencies
2. Fix breaking changes (required)
3. Adopt new features as needed (optional)

## Getting Help

- **Documentation**: See the full [v1.0.0 documentation](/docs)
- **Community**: Visit the [LangChain Forum](https://forum.langchain.com)
- **Issues**: Report problems on [GitHub](https://github.com/langchain-ai/langchain/issues)

## Example Migration

Here's a complete before/after example:

TODO















## Migration guide: v0 to v1

Migrating from v0 to v1 content formats can be done gradually or all at once.

### Migration strategies

<Tabs>
        <Tab title="Gradual Migration">
        Keep existing code unchanged and migrate incrementally:

        ```python
        # Step 1: Keep v0 models (default)
        model = ChatOpenAI()

        # Step 2: Use .content_blocks for new features
        response = model.invoke("Hello")
        v1_blocks = response.content_blocks  # Access v1 format

        # Step 3: Migrate components one at a time
        # Old code continues to work with .content
        legacy_text = response.content
        ```
        </Tab>
        <Tab title="Full Migration">
        Update all models and code to v1:

        ```python
        # Step 1: Set environment variable
        import os
        os.environ["LC_OUTPUT_VERSION"] = "v1"

        # Step 2: Update message handling
        response = model.invoke("Hello")
        blocks = response.content  # Now v1 format

        # Step 3: Update all message construction
        msg = AIMessage(content_blocks=[...])
        ```
        </Tab>
        <Tab title="Mixed Environment">
        Use both versions as needed:

        ```python
        # v1 for new features
        model_v1 = ChatOpenAI(output_version="v1")

        # v0 for legacy components
        model_v0 = ChatOpenAI(output_version="v0")

        # Handle both in shared utilities
        def process_message(msg):
            blocks = msg.content_blocks  # Always v1
            return blocks
        ```
        </Tab>
</Tabs>

### Common migration patterns

<AccordionGroup>
<Accordion title="Text content handling">
```python
# Before (v0)
if isinstance(response.content, str):
    text = response.content
elif isinstance(response.content, list):
    text = response.content[0] if response.content else ""

# After (v1-aware)
# Option 1: Use .text property
text = response.text

# Option 2: Extract from blocks
text_blocks = [
    b for b in response.content_blocks
    if b["type"] == "text"
]
text = text_blocks[0]["text"] if text_blocks else ""
```
</Accordion>

<Accordion title="Multimodal content">
```python
# Before (v0)
content = [
    "Analyze this:",
    {
        "type": "image",
        "source_type": "url",
        "url": "image.png"
    }
]

# After (v1)
from langchain_core.messages.content import (
    create_text_block,
    create_image_block
)

content_blocks = [
    create_text_block("Analyze this:"),
    create_image_block(url="image.png")
]

# Or using dict format
content_blocks = [
    {"type": "text", "text": "Analyze this:"},
    {"type": "image", "url": "image.png"}
]
```
</Accordion>

<Accordion title="Tool calls location">
```python
# v0: Tool calls in separate field
response = model_v0.invoke("Get weather")
if response.tool_calls:
    for tc in response.tool_calls:
        print(f"Tool: {tc['name']}")

# v1: Tool calls in content
response = model_v1.invoke("Get weather")
for block in response.content:
    if block["type"] == "tool_call":
        print(f"Tool: {block['name']}")
```
</Accordion>

<Accordion title="Message construction">
```python
# Before (multiple patterns)
msg1 = AIMessage("Hello")
msg2 = AIMessage(content="Hello")
msg3 = AIMessage(["Hello", "World"])

# After (v1 patterns)
from langchain_core.messages.content import create_text_block

# Option 1: content_blocks parameter
msg = AIMessage(content_blocks=[
    create_text_block("Hello")
])

# Option 2: Direct dict format
msg = AIMessage(content_blocks=[
    {"type": "text", "text": "Hello"}
])

# Backward compatible still works
msg = AIMessage("Hello")
```
</Accordion>
</AccordionGroup>

### Testing compatibility

Ensure your code works with both versions:

```python Test both versions
def test_version_compatibility():
    """Test v0 and v1 compatibility"""

    # Test with both versions
    for version in ["v0", "v1"]:
        model = ChatOpenAI(output_version=version)
        response = model.invoke("Hello")

        # Should work regardless of version
        assert response.text  # Text property
        assert response.content_blocks  # Always returns v1

        # Version-specific checks
        if version == "v1":
            assert isinstance(response.content, list)
            assert response.content[0]["type"] == "text"
        else:
            # v0 might be string or list
            assert response.content

def extract_text_universal(message):
    """Extract text regardless of version"""
    # Use content_blocks for universal access
    text_blocks = [
        b for b in message.content_blocks
        if b.get("type") == "text"
    ]

    return " ".join(b["text"] for b in text_blocks)
```

### Migration checklist

<Checklist>
- [ ] Identify all `response.content` usage in codebase
- [ ] Determine migration strategy (gradual/full/mixed)
- [ ] Update model initialization with `output_version` if needed
- [ ] Replace content access patterns with v1-aware code
- [ ] Update message construction to use `content_blocks`
- [ ] Add compatibility tests for both versions
- [ ] Update type hints to use `ContentBlock` types
- [ ] Document which version each component uses
- [ ] Set `LC_OUTPUT_VERSION` environment variable if doing full migration
- [ ] Test with actual provider responses
</Checklist>

<Warning>
**Important considerations:**
- v0 remains the default for backward compatibility
- Tool calls move from `.tool_calls` to `.content` in v1
- The `.content_blocks` property always returns v1 format
- Provider-specific fields go in the `extras` dict in v1
</Warning>
